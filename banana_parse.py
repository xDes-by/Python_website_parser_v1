# -*- coding: utf-8 -*-
from bs4 import BeautifulSoup
from urllib.request import urlopen
import re
import requests
import os
import json
from telegraph import Telegraph
import time, threading
import telebot
from config import TOKEN, URL, URL_2, IMGUR_TOKEN_1
from PIL import Image
import pyimgur

PARSE_DELAY = 600
DB_NAME = "parse.db"
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36"}

bot = telebot.TeleBot(TOKEN)

def get_site_content(html):
    print("–°—Ç–∞—Ä—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–∞")
    soup = BeautifulSoup(html.read(), "html.parser")
    items = soup.find_all('div', class_='main_post')
    items.pop(0)
    for item in items:
        article = item.find('span').text
        link = item.find('a').get('href')
        name_table = article.replace(" ", "_")
        name = re.sub("[^A-Za-z^–ê-–Ø–∞-—è_]", "", name_table)
        parse_article(link)


def get_article_content(html, link):
    article_id = link[-6:]
    soup = BeautifulSoup(html.read(), "html.parser")
    article_name = soup.find('div', class_='post_head').text
    article_name = check_name(article_name)
    articles = soup.find('div', id=f'news-id-{article_id}')
    if not os.path.exists(f"articles/banana/{article_id}"):
        print("–°—Ç–∞—Ä—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏")
        path = f"articles/banana/{article_id}/article"
        os.makedirs(path)
        print(f"Directory {article_id} created")
        with open(f"articles/banana/{article_id}/article/art.txt", 'w', encoding="utf-8"):
            print("File  created")
        for item in articles:
            a = item.text.split("\n")
            for k in a:
                if len(k) > 3:
                    news = re.sub(r'–ü–æ–¥ –∫–∞—Ç–æ–º', r'–í —Å—Ç–∞—Ç—å–µ', k)
                    with open(f"articles/banana/{article_id}/article/art.txt", 'a', encoding="utf-8") as file:
                        file.write(news + "\n")
        """—Å–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–æ–∫ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤ –Ω–∏—Ö –∫–∞—Ä—Ç–∏–Ω–æ–∫"""
        update_image(articles, article_id, article_name)

def check_name(article_name):
    ban_names = ["–ü–æ–ø–¥–±–æ—Ä–∫–∞", "–ü–æ–¥–±–æ—Ä–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ç–æ—Ä", "–°—Ç—Ä–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–Ω–æ—Å—Ç–∏", "Handbra", "–ü—Ä–∏–∫–æ–ª—å–Ω—ã–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏", "–£–ª–æ–≤ –∏–∑ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π", "–î–µ–≤—É—à–∫–∏ –∏–∑ –ó–∞–∑–µ—Ä–∫–∞–ª—å—è",
                 "–ú–æ–¥–Ω–∏–∫–∏ –≤ –º–µ—Ç—Ä–æ", "–ö—Ä–∞—Å–æ—Ç–∫–∏ –≤ –±–∏–∫–∏–Ω–∏", "–ö—Ä–∞—Å–∏–≤—ã–µ –æ—á–∫–∞—Ä–∏–∫–∏", "–≠—Ç–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è Butt Over Back", "–ò–¥–µ–∞–ª—å–Ω—ã–π –º–∏—Ä", "–†–∞—Å–ø–∏—Å–Ω—ã–µ –¥–µ–≤—É—à–∫–∏", "–°–ø–æ—Ä—Ç–∏–≤–Ω—ã–µ –¥–µ–≤—É—à–∫–∏",
                 "–°–æ–∫—Ä–æ–≤–∏—â–∞ –∏–∑ –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö –∫–æ–º–∏—Å—Å–∏–æ–Ω–æ–∫", "–ü–æ—à–ª—ã–µ –æ—Ç–∫—Ä–æ–≤–µ–Ω–∏—è", "–°–∞–º–æ–µ –Ω–∏–∂–Ω–µ–µ –±–µ–ª—å—ë", "–ù–µ—É–¥–∞—á–∏", "–ù—é–¥—Å–æ—á–µ—Ç–≤–µ—Ä–≥", "–ü–æ—à–ª—ã–µ —Å–µ–ª—Ñ–∏ —Å —Ä–∞–±–æ—Ç—ã", "–†–∞–∑–Ω–æ—Ü–≤–µ—Ç–Ω—ã–µ –≥–æ–ª–æ–≤—ã",
                 "–î–∂–∏–Ω—Å–æ–≤—ã–µ –ª–µ–¥–∏", "–î–æ—Ä–æ–≥–∞—è, —è –≤—Å–µ –ø–æ—á–∏–Ω–∏–ª —Å–∞–º!", "–ú—ã —Ö–æ—Ç–∏–º —Ç–∞—Ä–µ–ª–∫–∏!", "–í–æ–≤—Ä–µ–º—è!", "–ü–æ–ª–Ω—ã–π Underboob!", "–ì–æ—Ä—è—á–∏–µ –¥–µ–≤—É—à–∫–∏ –≤ –ª–∞—Ç–µ–∫—Å–µ –∏ –∫–æ–∂–µ", "–õ–∏–Ω–∏–∏ –∑–∞–≥–∞—Ä–∞",
                 "–ë–µ—Å–ø–æ–ª–µ–∑–Ω—ã–µ —Ñ–∞–∫—Ç—ã", "–ö—Ä–∞—Å–∏–≤—ã–µ –∞–∑–∏–∞—Ç–∫–∏", "–ú–∏–∫—Ä–æ—à–æ—Ä—Ç–∏–∫–∏", "–ü—è—Ç–Ω–∏—á–Ω—ã–µ —ç–π—Ä–±—ç–≥–∏" ]
    for i in ban_names:
        if article_name.find(i) != -1:
            article_name = i

    if not article_name.find("–®–∫–∞—Ä–ø—ç—Ç–∫—ñ, –ø–∞–Ω—á–æ—Ö—ñ"):
        article_name = "–ß—É–ª–∫–∏"
    print(article_name)
    return article_name

def update_image(articles, article_id, article_name):
    try:
        path = f"articles/banana/{article_id}/images"
        os.makedirs(path)
        z = articles.findAll("div")
        for i in z:
            link_image = i.find('img').get("src")
            p = requests.get(link_image)
            out = open(f"articles/banana/{article_id}/images/{link_image[-15:]}", "wb")  #—Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏
            out.write(p.content)

            # –æ–±—Ä–µ–∑–∞–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É
            img = Image.open(f"articles/banana/{article_id}/images/{link_image[-15:]}")
            size = img.size
            width, height = img.size
            new_image = img.crop((0, 0, width, height - 25))
            new_image.save(f'articles/banana/{article_id}/images/{link_image[-15:]}')

            im = pyimgur.Imgur(IMGUR_TOKEN_1)
            upload = im.upload_image(f"articles/banana/{article_id}/images/{link_image[-15:]}", title="any")
            d = i.find('img') # –ø–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –¥–ª—è –∫–∞—Ä—Ç–∏–Ω–∫–∏
            d['src'] = upload.link #–ø–æ–¥–º–µ–Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∏
            out.close()
    except:
        print("asdasd")

    create_article_for_post(articles, article_name, article_id)


def create_article_for_post(articles, article_name, article_id):
    a = re.sub(r'\<div id=.*\:inline;"', '<p', str(articles))
    x = a.replace("div", "p")
    x = x.replace("span", "p")
    # x = x.replace('<br/>', '\n')
    x = re.sub(r'–ü–æ–¥ –∫–∞—Ç–æ–º', r'–í —Å—Ç–∞—Ç—å–µ', x)
    youtube = re.findall(r'https://www.youtube.com/embed/\w+', x)
    if youtube:
        a = youtube[0]
        youtube_link = a.replace("https://www.youtube.com/embed/", "")
        x = re.sub(r'\<iframe.*\</iframe>',
                   f'<figure><iframe src="/embed/youtube?url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D{youtube_link}"></iframe></figure>',
                   x)
    create_news(article_name, x, article_id)


def create_news(head, main, article_id):
    telegraph = Telegraph()
    result = telegraph.create_account(short_name='1337')
    with open('graph_bot.json', 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=4)  # —Å–æ—Ö—Ä–∞–Ω—è—é –≤ —Ñ–∞–π–ª graph_bot
    with open('graph_bot.json') as f:
        graph_bot = json.load(f)
    telegraph = Telegraph(graph_bot["access_token"])  # –ø–µ—Ä–µ–¥–∞—ë–º —Ç–æ–∫–µ–Ω –¥–æ—Å—Ç—É–ø –∫ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º –∞–∫–∫–∞—É–Ω—Ç–∞s
    response = telegraph.create_page(
        f'{head}',  # –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        html_content=f"{main}"
    )
    print('https://telegra.ph/{}'.format(response['path']))  # —Ä–∞—Å–ø–µ—á–∞—Ç—ã–≤–∞–µ–º –∞–¥—Ä–µ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    a = 'https://telegra.ph/{}'.format(response['path'])  # —Ä–∞—Å–ø–µ—á–∞—Ç—ã–≤–∞–µ–º –∞–¥—Ä–µ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    # bot.send_message(534738342, f"[üç≥]({a}) {head} ", parse_mode="Markdown") #–∏–≤–∞–Ω
    # bot.send_message(1145437985, f"[üç≥]({a}) {head} ", parse_mode="Markdown") #—è


def parse_site():
    try:
        html = urlopen(URL,  timeout=10)
        get_site_content(html)
    except Exception as e:
        print(e)
        print("error site parse")
    threading.Timer(PARSE_DELAY, parse_site).start()


def parse_article(link):
    try:
        html = urlopen(link,  timeout=10)
        print(type(html))
        get_article_content(html, link)
    except Exception as e:
        print(e)
        print("error article parse")

parse_site()

while True:
    try:
        bot = telebot.TeleBot(TOKEN, parse_mode=None)
        bot.polling(none_stop=True)
    except Exception as e:
        print(e)
        time.sleep(15)
